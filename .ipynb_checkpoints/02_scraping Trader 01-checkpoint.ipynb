{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape cars on sale on various trader websites\n",
    "\n",
    "The code below scrape cars available on autotrader.com for given zipcodes, and store them in a sql databases for further analyses. This particular trader seems to be actively fighting scraping of its offers by regularly modifying the html format and css files and class names. \n",
    "\n",
    "Issues right now:\n",
    "* List is limited to top 1000 results, in this case only most expensive cars, so results are _clearly biased_.\n",
    "* I stopped the script after 10 zipcodes in TX. (~10000 results).\n",
    "* Script is slow. Using selenium to go through all search result pages. Not ideal.\n",
    "* Not using proxies. I'm assuming that autotrader will ban me at some point. \n",
    "* Also, _Site using multiple css templates that render the same (possibly to hinder scraping...). This script only works with one of them and results in an error when the wrong one is loaded._ Next step would be to write a script for the other template and automatically identify which css template is used.\n",
    "* Because I'm searching in a 25miles radius in each zipcode, the same cars can show up several time. For now I'm dealing with this with a SQL command after scraping. \n",
    "\n",
    "Possible solutions:\n",
    "* Identify how results are transfered and intercept AJAX/JSON requests..?\n",
    "* write a script for the other css templates used and automatically identify which is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import time\n",
    "\n",
    "%load_ext sql\n",
    "%config SqlMagic.autopandas=True\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %qtconsole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Connect to dbs\n",
    "conn = sqlite3.connect('data/sales.db')\n",
    "%sql sqlite:///data/zipcode.sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get list of zipcodes, only do TX for now (LIMIT 10 because super slow... \n",
    "zips = %sql select ZipCode from zipcode where St = 'TX' limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trader = 'http://www.autotrader.com'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for z in zips.ZipCode:\n",
    "    \n",
    "    print(z)\n",
    "\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(trader)\n",
    "\n",
    "#     elem = browser.find_element_by_name('fyc-form-j_id_bb-j_id_bn-j_id_cp-zipcode')  # Find the search box\n",
    "    elem = browser.find_element_by_name('fyc-form-j_id_be-j_id_bq-j_id_cs-zipcode')  # Find the search box\n",
    "    elem.send_keys(Keys.ESCAPE)\n",
    "    elem.send_keys(str(z))\n",
    "    elem.send_keys(Keys.RETURN)\n",
    "    time.sleep(20)    \n",
    "\n",
    "    not_end = 0\n",
    "    while not not_end:\n",
    "        soup = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "\n",
    "        # Create DataFrame\n",
    "        # columns = ['NewUsed', 'Year', 'Make', 'Type','Mileage','Price','ZipCode','Source']\n",
    "        all_cars = pd.DataFrame() #columns=columns)\n",
    "\n",
    "        #Make and Type\n",
    "        brands = soup.find_all(\"span\", class_=\"atcui-truncate ymm\")\n",
    "        #Price and Mileage\n",
    "        details = soup.find_all(\"div\", class_=\"atcui-section atcui-clearfix   listing-content \")\n",
    "\n",
    "        for cars, entry in zip(brands, details):\n",
    "            carsSTR = cars.text.split()\n",
    "            car_nu = carsSTR[0]\n",
    "            car_year = int(carsSTR[1])\n",
    "            car_make = carsSTR[2]\n",
    "            car_type = \" \".join(carsSTR[3:])\n",
    "\n",
    "            try:\n",
    "                milesSTR = entry.find_all(\"span\", class_=\"mileage\")[0].text\n",
    "                car_miles = int(milesSTR.split()[0].replace(',',''))\n",
    "            except:\n",
    "                car_miles = 'Null'\n",
    "\n",
    "            try:\n",
    "                priceSTR = entry.find_all(\"h4\", class_=\"primary-price\")[0].text\n",
    "                car_price = int(priceSTR.replace('$','').replace(',',''))\n",
    "            except:\n",
    "                car_price = 'Null'\n",
    "\n",
    "\n",
    "    #         print('{} {} {} {} {} {}'.format(car_year, car_nu, car_make, car_type, car_price, car_miles))\n",
    "            all_cars = all_cars.append(pd.DataFrame([[car_year, car_nu, car_make, \n",
    "                                                      car_type, car_miles, car_price, \n",
    "                                                      z, trader]]),\n",
    "                                      ignore_index = True)\n",
    "\n",
    "        all_cars.columns = ['new_used', 'year', 'make', 'type', 'mileage', 'price', 'zipcode', 'source']    \n",
    "        # Update index\n",
    "        index_max = conn.execute('SELECT MAX(car_id) FROM sales').fetchall()\n",
    "        if index_max[0][0] is not None:\n",
    "            all_cars.index = all_cars.index + 1 + index_max[0][0]\n",
    "        all_cars.to_sql('sales',conn,flavor='sqlite',if_exists='append',index_label='car_id')\n",
    "\n",
    "        next_counter = 0\n",
    "        while next_counter < 4:\n",
    "            try: \n",
    "                browser.find_element_by_class_name('pagination-button-next').click()\n",
    "                next_counter = 4\n",
    "            except:\n",
    "                if next_counter == 3:\n",
    "                    not_end = 1\n",
    "                time.sleep(2)\n",
    "                next_counter = next_counter + 1\n",
    "\n",
    "    browser.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
